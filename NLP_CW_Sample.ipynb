{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_CW_Sample.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sC9PZgxcMd_F"},"source":["# 1. import data into a DataFrame \r\n","#     pd.read_csv(PATH/file.csv)  \r\n","# 2. Tokenize textual data, extract features, convert them into vectors\r\n","# 3. Modeling, train models on training set (select model, tune different parameters)\r\n","# 4. Evaluation on dev set (metrics calculation, error analysis)\r\n","# 5. Prediction on test set (store your results in given format and submit it)"]},{"cell_type":"markdown","metadata":{"id":"934uDa63sRnp"},"source":["# Step 1: Load Dataset\n"]},{"cell_type":"code","metadata":{"id":"cxDjTTcDrtVl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614888654663,"user_tz":0,"elapsed":10861,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"f7253c2a-7c22-40c6-d7ed-e613610d9ac6"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","\n","#  Load the data set\n","#FILEPATH = \"dir/dev.csv\"\n","#df = pd.read_csv(FILEPATH)\n","#df_dev = \n","#df_test = \n","# pd.read_csv('/content/drive/MyDrive/PY/NLP/4groupNews.csv')\n","categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n","from sklearn.datasets import fetch_20newsgroups\n","twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading 20news dataset. This may take a few minutes.\n","Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"XDFYXo9uV6ZS","executionInfo":{"status":"ok","timestamp":1614889452739,"user_tz":0,"elapsed":538,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"19975091-62ff-4c9a-820c-f291f0e8766e"},"source":["df = pd.DataFrame({'text':twenty_train.data,'class':twenty_train.target})\r\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>From: sd345@city.ac.uk (Michael Collier)\\nSubj...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  class\n","0  From: sd345@city.ac.uk (Michael Collier)\\nSubj...      1\n","1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...      1\n","2  From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...      3\n","3  From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...      3\n","4  From: stanly@grok11.columbiasc.ncr.com (stanly...      3"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"44xTvpLa_UC9"},"source":["# Step 2: Build a Pipeline"]},{"cell_type":"code","metadata":{"id":"HQzwithdCl9e","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1614889586881,"user_tz":0,"elapsed":478,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"24ba1a9a-1d66-4e04-b676-a2d7e47fbf2f"},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import SGDClassifier, LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.stem.snowball import EnglishStemmer\n","\n","def stemmed_words(doc):\n","    return (stemmer.stem(w) for w in analyzer(doc))\n","\n","# Level: lexicom, model: tf-idf\n","text_clf = Pipeline([\n","    # add your code about text processing           \n","    ('vect', CountVectorizer(analyzer=stemmed_words)), \n","    ('tfidf', TfidfTransformer()),\n","\n","    # change your classifier here, for example naive bayes classifier\n","    ('clf', MultinomialNB())\n","\n","])\n","# show example of tf-dif weighting\n","TfidfTransformer(twenty_train.data[1])\n","\n","# To train the model with your own data set \n","text_clf.fit(twenty_train.data, twenty_train.target)\n","\n","# To make prediction with dev/test set\n","predicted = text_clf.predict(twenty_test.data)\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-228c6ce6af1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'subject_sentences' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"3GXHJHqoBmyJ"},"source":["# Step 3: Evaluation"]},{"cell_type":"code","metadata":{"id":"LdB9js0QDErf"},"source":["# To evaluate your prediction on dev set\n","from sklearn import metrics\n","print(\"Accuracy:\", metrics.accuracy_score(twenty_test.target, predicted))\n","\n","print(\"F1:\", metrics.f1_score(predicted == twenty_test.target))\n","\n","print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n","\n","# confusion class\n","print(pd.DataFrame(metrics.confusion_matrix(twenty_test.target, predicted),\n","             columns=twenty_test.target_names,index=twenty_test.target_names))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCLCqFXPQsRq"},"source":["# Step 4: Error Analysis"]},{"cell_type":"code","metadata":{"id":"kvBw9qkKDS-m"},"source":["dev_true = twenty_test.target # a list of true labels\n","\n","# df_dev = pd.DataFrame(devtest_names,columns=['name','true gender'])\n","# df_dev['pred gender'] = predicted\n","# df_dev[df_dev['true gender'] != df_dev['pred gender']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZnKjlO5RlLp"},"source":["# Other code you can use"]},{"cell_type":"code","metadata":{"id":"tWZB6bD-RriL"},"source":["# Logistic Regression Model:  ('clf', LogisticRegression())\r\n","\r\n","# "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mu-whDxMUWRU"},"source":["# add stemmer into pipeline\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","from nltk.stem.snowball import EnglishStemmer\r\n","\r\n","stemmer = EnglishStemmer()\r\n","analyzer = CountVectorizer().build_analyzer()\r\n","\r\n","def stemmed_words(doc):\r\n","    return (stemmer.stem(w) for w in analyzer(doc))\r\n","\r\n","stem_vectorizer = CountVectorizer(analyzer=stemmed_words)\r\n","print(stem_vectorizer.fit_transform(['Tu marches dans la rue']))\r\n","print(stem_vectorizer.get_feature_names())"],"execution_count":null,"outputs":[]}]}