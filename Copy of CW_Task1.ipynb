{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CW_Task1.ipynb","provenance":[{"file_id":"15ZvXDbGU32-d3afddksL6uCbAarx-gl3","timestamp":1615308506134},{"file_id":"1I3g5DeuhS0sSOBd4DeQhR7OwjaSSx6gy","timestamp":1615032664939},{"file_id":"1lTmqMvgIU4LIna3kklDa8J7UncnycGCy","timestamp":1615030669644}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BgMimfxBvPUW"},"source":["# Note: Personal attacks are NEVER acceptable. Show your respect, understanding, and empathy. Remember that you are civilized and educated. Improve your communication: always directly let your lecturer know that you are struggling on something as early as possible. \n","\n","Example sentence: I spent X time on Y. I'm struggling at Z. Can you give me some suggestions or guidence? \n","\n","Good example questions from the chat: \n","\n","1) How to load the dataset? \n","\n","2) I didnt really get it either. the triangle stuff. \n","\n","3) Are there any examples we can look at for how to change it for other linguistic levels?\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"QXdrc29D71MH"},"source":["# Steps:\r\n","1. import data into a DataFrame\r\n","pd.read_csv(PATH/file.csv)\r\n","2. Process your text data, extract features, convert them into vectors\r\n","3. Modeling, train models on training set (select model, tune different parameters)\r\n","4. Evaluation on dev/test set \r\n","5. Prediction on test set "]},{"cell_type":"markdown","metadata":{"id":"934uDa63sRnp"},"source":["# Step 1: Load Dataset\n"]},{"cell_type":"code","metadata":{"id":"cxDjTTcDrtVl"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","\n","\n","categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n","from sklearn.datasets import fetch_20newsgroups\n","twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n","twenty_test = fetch_20newsgroups(subset='test',categories=categories, shuffle=True, random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1jAHpjtaSPu"},"source":["# 1.1 Examples of NLP preprocessing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6hPeXYWabCc","executionInfo":{"elapsed":740,"status":"ok","timestamp":1615207514597,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"},"user_tz":0},"outputId":"198a8348-f5a3-4f97-deeb-7b3434d79ab9"},"source":["# write your own NLP precessing examples with your own preprocessing techniques.\r\n","# check lecture slides for the NLP techniques. \r\n","\r\n","print(twenty_train.data[1])\r\n","import nltk\r\n","nltk.download('punkt')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","nltk.download('wordnet')\r\n","\r\n","# tokenize: search: nltk tokenize \r\n","example = \"This is an example sentence.\"\r\n","\r\n","from nltk.tokenize import word_tokenize\r\n","example_tokenize= word_tokenize(example) #twenty_train.data[1]\r\n","print(\"-------------------------tokenize:\")\r\n","print(example_tokenize)\r\n","\r\n","\r\n","# stemmer: search: nltk stemmer  \r\n","stemmer = nltk.stem.PorterStemmer()\r\n","example_stem = stemmer.stem(example)\r\n","print(\"-------------------------stem:\")\r\n","print(example_stem)\r\n","\r\n","# tf-idf: search: scikit learn tf-idf https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\r\n","# search scikit learn CountVectorizer example: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \r\n","\r\n","# pos_taging: search: nltk pos tagging example\r\n","example_posTag=nltk.pos_tag(example_tokenize)\r\n","print(\"-------------------------pos_taging:\")\r\n","print(example_posTag)\r\n","\r\n","# consituency parsing, chunking\r\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\"\r\n","cp = nltk.RegexpParser(grammar)\r\n","result = cp.parse(example_posTag)\r\n","print(result)\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n","Subject: help: Splitting a trimming region along a mesh \n","Organization: University Of Kentucky, Dept. of Math Sciences\n","Lines: 28\n","\n","\n","\n","\tHi,\n","\n","\tI have a problem, I hope some of the 'gurus' can help me solve.\n","\n","\tBackground of the problem:\n","\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n","\tmapping of a 3d Bezier patch into 2d. The area in this domain\n","\twhich is inside a trimming loop had to be rendered. The trimming\n","\tloop is a set of 2d Bezier curve segments.\n","\tFor the sake of notation: the mesh is made up of cells.\n","\n","\tMy problem is this :\n","\tThe trimming area has to be split up into individual smaller\n","\tcells bounded by the trimming curve segments. If a cell\n","\tis wholly inside the area...then it is output as a whole ,\n","\telse it is trivially rejected. \n","\n","\tDoes any body know how thiss can be done, or is there any algo. \n","\tsomewhere for doing this.\n","\n","\tAny help would be appreciated.\n","\n","\tThanks, \n","\tAni.\n","-- \n","To get irritated is human, to stay cool, divine.\n","\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","-------------------------tokenize:\n","['This', 'is', 'an', 'example', 'sentence', '.']\n","-------------------------stem:\n","this is an example sentence.\n","-------------------------pos_taging:\n","[('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('.', '.')]\n","(S This/DT is/VBZ (NP an/DT example/NN) (NP sentence/NN) ./.)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c7VpnVNpKuUt"},"source":["#1.2 NLP Preprocesssing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B8u5y9adK3tc","executionInfo":{"elapsed":3827,"status":"ok","timestamp":1615295729966,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"},"user_tz":0},"outputId":"324230e1-3ba1-4a3f-8bd3-9b77f7a64e29"},"source":["# define your own functions of NLP pre-processing, e.g.: pos tagging, n-gram, parsing, chunking, get word semantics of wordNet\r\n","# Tips for first class: your own functions are creative and effective, which reflecting your deep understanding and insight of these techniques. \r\n","\r\n","import nltk\r\n","import re\r\n","from nltk.tokenize import sent_tokenize, word_tokenize\r\n","nltk.download('stopwords')\r\n","nltk.download('punkt')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","from tqdm import tqdm\r\n","from nltk.corpus import stopwords\r\n","stopwordEn = stopwords.words('english')\r\n","from nltk.corpus import wordnet\r\n","nltk.download('wordnet')\r\n","from nltk.stem.snowball import SnowballStemmer\r\n","stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\r\n","\r\n","def lemmaWord(word):\r\n","    lemma = wordnet.morphy(word)\r\n","    if lemma is not None:\r\n","        return lemma\r\n","    else:\r\n","        return word\r\n","\r\n","def stemWord(word):\r\n","    stem = stemmer.stem(word)\r\n","    if stem is not None:\r\n","        return stem\r\n","    else:\r\n","        return word\r\n","\r\n","def processText(text,lemma=False, gram=1, rmStop=True): # default remove stop words\r\n","    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE) #delete URL, #hashtag# , and @xxx\r\n","    tokens = word_tokenize(text)\r\n","    whitelist = [\"n't\", \"not\", \"no\"]\r\n","    new_tokens = []\r\n","    stoplist = stopwordEn if rmStop else []\r\n","    for i in tokens:\r\n","      i = i.lower()\r\n","      if i.isalpha() and (i not in stoplist or i in whitelist):  #i not in ['.',',',';']  and (...)\r\n","        if lemma: i = lemmaWord(i)\r\n","        new_tokens.append(i)\r\n","    del tokens\r\n","    # tokens = [lemmaWord(i.lower()) if lemma else i.lower() for i in tokens if (i.lower() not in stoplist or i.lower() in whitelist) and i.isalpha()]\r\n","    if gram<=1:\r\n","        return new_tokens\r\n","    else:\r\n","        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]\r\n","\r\n","       "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YKFoZaWSVrnq"},"source":["def getTags(text):\r\n","  token = word_tokenize(text)\r\n","  token = [l.lower() for l in token]\r\n","  train_tags = nltk.pos_tag(token)\r\n","  return [i[1] for i in train_tags]\r\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8mwYOcFcS02","executionInfo":{"elapsed":481,"status":"ok","timestamp":1615033008677,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"},"user_tz":0},"outputId":"07c25cd1-b39f-4e1e-90f0-1ce29f0e1be3"},"source":["print(processText(twenty_train.data[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['michael', 'collier', 'subject', 'converting', 'images', 'hp', 'laserjet', 'iii', 'hampton', 'organization', 'city', 'university', 'lines', 'anyone', 'know', 'good', 'way', 'standard', 'pc', 'utility', 'convert', 'files', 'laserjet', 'iii', 'format', 'would', 'also', 'like', 'converting', 'hpgl', 'hp', 'plotter', 'files', 'please', 'email', 'response', 'correct', 'group', 'thanks', 'advance', 'michael', 'michael', 'collier', 'programmer', 'computer', 'unit', 'email', 'city', 'university', 'tel', 'london', 'fax']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZRqySa8cbr1","executionInfo":{"elapsed":468,"status":"ok","timestamp":1615033011368,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"},"user_tz":0},"outputId":"7967103b-d467-4162-ce35-f546bcfe1939"},"source":["print(getTags(twenty_train.data[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['IN', ':', 'NN', 'NN', 'NN', '(', 'FW', 'NN', ')', 'NN', ':', 'NN', 'NNS', 'TO', 'VB', 'NN', 'NN', '.', 'NN', ':', 'NN', 'NN', ':', 'DT', 'NN', 'NN', 'NNS', ':', 'CD', 'VBZ', 'NN', 'VB', 'IN', 'DT', 'JJ', 'NN', '(', 'JJ', 'NN', 'JJ', 'NN', ')', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'JJ', 'NN', '.', 'PRP', 'MD', 'RB', 'VB', 'TO', 'VB', 'DT', 'JJ', ',', 'VBG', 'TO', 'VB', '(', 'JJ', 'NN', ')', 'NNS', '.', 'VB', 'JJ', 'DT', 'NN', '.', 'VBZ', 'DT', 'DT', 'JJ', 'NN', '.', 'NNS', 'IN', 'NN', '.', 'NN', '.', ':', 'JJ', 'NN', '(', 'NN', ')', 'DT', 'NN', 'NN', ',', 'NN', ':', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'NN', ',', 'NN', ':', 'CD', 'CD', 'JJ', 'NN', ',', 'NN', ':', 'CD', 'JJ', 'NN', 'CD', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"44xTvpLa_UC9"},"source":["# Step 2: Build a Pipeline"]},{"cell_type":"code","metadata":{"id":"vNm3axlhdzlF"},"source":["from sklearn.pipeline import Pipeline\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","from sklearn.feature_extraction.text import TfidfTransformer\r\n","from sklearn.linear_model import SGDClassifier, LogisticRegression\r\n","\r\n","# Level: lexicon, model: tf-idf\r\n","text_clf = Pipeline([\r\n","    # add your code about text processing           \r\n","    ('vect', CountVectorizer(analyzer=processText)), \r\n","    ('tfidf', TfidfTransformer(use_idf=True)),\r\n","\r\n","    # change your classifier here, search: sklearn logistic regression example\r\n","    ('clf', SGDClassifier())\r\n","\r\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vuq37Bf3Qjpn","executionInfo":{"elapsed":8970,"status":"ok","timestamp":1615305702891,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"},"user_tz":0},"outputId":"d4de5db1-4fa9-432a-bb20-25cef3b6e606"},"source":["# To train the model \r\n","text_clf.fit(twenty_train.data, twenty_train.target)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer=<function processText at 0x7f4ac55dba70>,\n","                                 binary=False, decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b...\n","                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n","                               early_stopping=False, epsilon=0.1, eta0=0.0,\n","                               fit_intercept=True, l1_ratio=0.15,\n","                               learning_rate='optimal', loss='hinge',\n","                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n","                               penalty='l2', power_t=0.5, random_state=None,\n","                               shuffle=True, tol=0.001, validation_fraction=0.1,\n","                               verbose=0, warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"gjQ8DmPNRUuJ"},"source":["# Step 3: Make Prediction"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"OMdoIHjMRWce"},"source":["# To make prediction with dev/test set\r\n","predicted = text_clf.predict(twenty_test.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GXHJHqoBmyJ"},"source":["# Step 4: Evaluation"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"id":"LdB9js0QDErf","executionInfo":{"elapsed":931,"status":"ok","timestamp":1615295787422,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"},"user_tz":0},"outputId":"7fbeaa50-fc26-4ee4-c724-ba8dd2b507ba"},"source":["# To evaluate your prediction on dev set\n","from sklearn import metrics\n","print(\"Accuracy:\", metrics.accuracy_score(twenty_test.target, predicted))\n","\n","print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n","\n","# confusion class\n","pd.DataFrame(metrics.confusion_matrix(twenty_test.target, predicted),\n","             columns=twenty_test.target_names,index=twenty_test.target_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.9127829560585885\n","                        precision    recall  f1-score   support\n","\n","           alt.atheism       0.93      0.80      0.86       319\n","         comp.graphics       0.92      0.97      0.94       389\n","               sci.med       0.94      0.92      0.93       396\n","soc.religion.christian       0.87      0.94      0.91       398\n","\n","              accuracy                           0.91      1502\n","             macro avg       0.92      0.91      0.91      1502\n","          weighted avg       0.91      0.91      0.91      1502\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>alt.atheism</th>\n","      <th>comp.graphics</th>\n","      <th>sci.med</th>\n","      <th>soc.religion.christian</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>alt.atheism</th>\n","      <td>255</td>\n","      <td>6</td>\n","      <td>12</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>comp.graphics</th>\n","      <td>4</td>\n","      <td>377</td>\n","      <td>7</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>sci.med</th>\n","      <td>6</td>\n","      <td>19</td>\n","      <td>364</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>soc.religion.christian</th>\n","      <td>9</td>\n","      <td>8</td>\n","      <td>6</td>\n","      <td>375</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        alt.atheism  ...  soc.religion.christian\n","alt.atheism                     255  ...                      46\n","comp.graphics                     4  ...                       1\n","sci.med                           6  ...                       7\n","soc.religion.christian            9  ...                     375\n","\n","[4 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"qCLCqFXPQsRq"},"source":["# Step 5: Error Analysis and Discussion\n","write down your own obseration about the predictions. Consider both confusion matrix and selected examples. Which classes are predicted correctly or incorrecly, possible explaination, possible solutions \n","\n","Exmaple: 1) Practical 3, which feature is helpful for female name classification. https://www.nltk.org/book/ch06.html \n","2) research paper: https://github.com/yoonkim/CNN_sentence \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"kvBw9qkKDS-m","executionInfo":{"elapsed":739,"status":"ok","timestamp":1615207165451,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"},"user_tz":0},"outputId":"ff41b657-1add-4633-838d-79fb63856808"},"source":["df_pred = pd.DataFrame({'news':twenty_test.data,'prediction':predicted, 'true':twenty_test.target})\n","df_pred[df_pred['true'] != df_pred['prediction']]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>news</th>\n","      <th>prediction</th>\n","      <th>true</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12</th>\n","      <td>From: \"Gabriel D. Underwood\" &lt;gabe+@CMU.EDU&gt;\\n...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>From: swf@elsegundoca.ncr.com (Stan Friesen)\\n...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: D...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>From: UC512052@mizzou1.missouri.edu (David K. ...</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>Organization: Penn State University\\nFrom: &lt;RF...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1438</th>\n","      <td>From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1450</th>\n","      <td>From: alan.barclay@almac.co.uk (Alan Barclay)\\...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1455</th>\n","      <td>From: pww@spacsun.rice.edu (Peter Walker)\\nSub...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1480</th>\n","      <td>From: wilsonr@logica.co.uk\\nSubject: Re: What ...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1492</th>\n","      <td>From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>126 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                   news  prediction  true\n","12    From: \"Gabriel D. Underwood\" <gabe+@CMU.EDU>\\n...           3     2\n","15    From: swf@elsegundoca.ncr.com (Stan Friesen)\\n...           2     0\n","19    From: mathew <mathew@mantis.co.uk>\\nSubject: D...           3     0\n","26    From: UC512052@mizzou1.missouri.edu (David K. ...           2     1\n","36    Organization: Penn State University\\nFrom: <RF...           0     2\n","...                                                 ...         ...   ...\n","1438  From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...           0     3\n","1450  From: alan.barclay@almac.co.uk (Alan Barclay)\\...           1     2\n","1455  From: pww@spacsun.rice.edu (Peter Walker)\\nSub...           3     0\n","1480  From: wilsonr@logica.co.uk\\nSubject: Re: What ...           3     0\n","1492  From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...           3     0\n","\n","[126 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"DRIRg2rufjPu"},"source":["#References:  Practical 3\n","\n","https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html \n","\n","https://www.nltk.org/book/ch06.html \n","\n","search: Other online resources: \n","\n","https://towardsdatascience.com/setting-up-text-preprocessing-pipeline-using-scikit-learn-and-spacy-e09b9b76758f \n"," \n","sentiment analysis scikit learn \n","\n","scikit learn or nltk + NLP techniques \n","\n","python + NLP techniques\n","\n","scikit learn logistic regression\n","\n","\n"]}]}