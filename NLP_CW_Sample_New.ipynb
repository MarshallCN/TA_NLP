{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_CW_Sample_New.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QXdrc29D71MH"},"source":["\r\n","1. import data into a DataFrame\r\n","pd.read_csv(PATH/file.csv)\r\n","2. Tokenize textual data, extract features, convert them into vectors\r\n","3. Modeling, train models on training set (select model, tune different parameters)\r\n","4. Evaluation on dev set (metrics calculation, error analysis)\r\n","5. Prediction on test set (store your results in given format and submit it)"]},{"cell_type":"markdown","metadata":{"id":"934uDa63sRnp"},"source":["# Step 1: Load Dataset\n"]},{"cell_type":"code","metadata":{"id":"cxDjTTcDrtVl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615030632169,"user_tz":0,"elapsed":14263,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"3ac0f963-5d48-406f-f94e-2b3ce910503d"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import pandas as pd\n","import numpy as np\n","\n","#  Load the data set\n","#FILEPATH = \"dir/dev.csv\"\n","#df = pd.read_csv(FILEPATH) \n","#df_dev = \n","#df_test = \n","categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n","from sklearn.datasets import fetch_20newsgroups\n","twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)\n","twenty_test = fetch_20newsgroups(subset='test',categories=categories, shuffle=True, random_state=42)\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading 20news dataset. This may take a few minutes.\n","Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"v1jAHpjtaSPu"},"source":["# Examples of NLP analysis"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W6hPeXYWabCc","executionInfo":{"status":"ok","timestamp":1615031481731,"user_tz":0,"elapsed":382,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"c57344e8-7b77-4a1d-c62a-2edd240bb01f"},"source":["print(twenty_train.data[1])\r\n","import nltk\r\n","nltk.download('punkt')\r\n","#nltk.download('averaged_perceptron_tagger')\r\n","#nltk.download('wordnet')\r\n","\r\n","# tokenize: search: nltk tokenize \r\n","example = \"This is an example sentence.\"\r\n","\r\n","from nltk.tokenize import word_tokenize\r\n","example_tokenize= word_tokenize(twenty_train.data[1])\r\n","print(\"-------------------------tokenize:\")\r\n","print(example_tokenize)\r\n","\r\n","\r\n","# stemmer: search: nltk stemmer example, or coreNLP stemmer\r\n","stemmer = nltk.stem.PorterStemmer()\r\n","example_stem = stemmer.stem(twenty_train.data[1])\r\n","print(\"-------------------------stem:\")\r\n","print(example_stem)\r\n","\r\n","# tf-idf: search: scikit-learn tf-idf example  https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\r\n","\r\n","\r\n","\r\n","# pos_taging search: nltk pos tagging \r\n","\r\n","# *** example_posTag=nltk.pos_tag(twenty_train.data[1])\r\n","# *** print(\"-------------------------pos_taging:\")\r\n","# *** print(example_posTag)\r\n","# *** separator = ', '\r\n","# *** example_clearning= separator.join(example_posTag[1])\r\n","# *** print(example_clearning)\r\n","\r\n","# consituency parsing: search nltk parsing example\r\n","\r\n","# *** grammar = \"NP: {<DT>?<JJ>*<NN>}\"\r\n","# *** cp = nltk.RegexpParser(grammar)\r\n","# *** result = cp.parse(example_posTag)\r\n","# *** print(result[2])\r\n","\r\n","#  word semantics: wordNet: \r\n","\r\n","# *** from nltk.corpus import wordnet\r\n","# *** from nltk.corpus import wordnet as wn\r\n","# *** print (wn.synsets('example'))\r\n","# *** for ss in wn.synsets('example'):\r\n","# ***    print(ss.name(),ss.lemma_names())\r\n","\r\n","# other NLP techniques: word embedding\r\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n","Subject: help: Splitting a trimming region along a mesh \n","Organization: University Of Kentucky, Dept. of Math Sciences\n","Lines: 28\n","\n","\n","\n","\tHi,\n","\n","\tI have a problem, I hope some of the 'gurus' can help me solve.\n","\n","\tBackground of the problem:\n","\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n","\tmapping of a 3d Bezier patch into 2d. The area in this domain\n","\twhich is inside a trimming loop had to be rendered. The trimming\n","\tloop is a set of 2d Bezier curve segments.\n","\tFor the sake of notation: the mesh is made up of cells.\n","\n","\tMy problem is this :\n","\tThe trimming area has to be split up into individual smaller\n","\tcells bounded by the trimming curve segments. If a cell\n","\tis wholly inside the area...then it is output as a whole ,\n","\telse it is trivially rejected. \n","\n","\tDoes any body know how thiss can be done, or is there any algo. \n","\tsomewhere for doing this.\n","\n","\tAny help would be appreciated.\n","\n","\tThanks, \n","\tAni.\n","-- \n","To get irritated is human, to stay cool, divine.\n","\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","-------------------------tokenize:\n","['From', ':', 'ani', '@', 'ms.uky.edu', '(', 'Aniruddha', 'B.', 'Deglurkar', ')', 'Subject', ':', 'help', ':', 'Splitting', 'a', 'trimming', 'region', 'along', 'a', 'mesh', 'Organization', ':', 'University', 'Of', 'Kentucky', ',', 'Dept', '.', 'of', 'Math', 'Sciences', 'Lines', ':', '28', 'Hi', ',', 'I', 'have', 'a', 'problem', ',', 'I', 'hope', 'some', 'of', 'the', \"'gurus\", \"'\", 'can', 'help', 'me', 'solve', '.', 'Background', 'of', 'the', 'problem', ':', 'I', 'have', 'a', 'rectangular', 'mesh', 'in', 'the', 'uv', 'domain', ',', 'i.e', 'the', 'mesh', 'is', 'a', 'mapping', 'of', 'a', '3d', 'Bezier', 'patch', 'into', '2d', '.', 'The', 'area', 'in', 'this', 'domain', 'which', 'is', 'inside', 'a', 'trimming', 'loop', 'had', 'to', 'be', 'rendered', '.', 'The', 'trimming', 'loop', 'is', 'a', 'set', 'of', '2d', 'Bezier', 'curve', 'segments', '.', 'For', 'the', 'sake', 'of', 'notation', ':', 'the', 'mesh', 'is', 'made', 'up', 'of', 'cells', '.', 'My', 'problem', 'is', 'this', ':', 'The', 'trimming', 'area', 'has', 'to', 'be', 'split', 'up', 'into', 'individual', 'smaller', 'cells', 'bounded', 'by', 'the', 'trimming', 'curve', 'segments', '.', 'If', 'a', 'cell', 'is', 'wholly', 'inside', 'the', 'area', '...', 'then', 'it', 'is', 'output', 'as', 'a', 'whole', ',', 'else', 'it', 'is', 'trivially', 'rejected', '.', 'Does', 'any', 'body', 'know', 'how', 'thiss', 'can', 'be', 'done', ',', 'or', 'is', 'there', 'any', 'algo', '.', 'somewhere', 'for', 'doing', 'this', '.', 'Any', 'help', 'would', 'be', 'appreciated', '.', 'Thanks', ',', 'Ani', '.', '--', 'To', 'get', 'irritated', 'is', 'human', ',', 'to', 'stay', 'cool', ',', 'divine', '.']\n","-------------------------stem:\n","from: ani@ms.uky.edu (aniruddha b. deglurkar)\n","subject: help: splitting a trimming region along a mesh \n","organization: university of kentucky, dept. of math sciences\n","lines: 28\n","\n","\n","\n","\thi,\n","\n","\ti have a problem, i hope some of the 'gurus' can help me solve.\n","\n","\tbackground of the problem:\n","\ti have a rectangular mesh in the uv domain, i.e  the mesh is a \n","\tmapping of a 3d bezier patch into 2d. the area in this domain\n","\twhich is inside a trimming loop had to be rendered. the trimming\n","\tloop is a set of 2d bezier curve segments.\n","\tfor the sake of notation: the mesh is made up of cells.\n","\n","\tmy problem is this :\n","\tthe trimming area has to be split up into individual smaller\n","\tcells bounded by the trimming curve segments. if a cell\n","\tis wholly inside the area...then it is output as a whole ,\n","\telse it is trivially rejected. \n","\n","\tdoes any body know how thiss can be done, or is there any algo. \n","\tsomewhere for doing this.\n","\n","\tany help would be appreciated.\n","\n","\tthanks, \n","\tani.\n","-- \n","to get irritated is human, to stay cool, divine.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c7VpnVNpKuUt"},"source":["# Preprocesssing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B8u5y9adK3tc","executionInfo":{"status":"ok","timestamp":1615032028598,"user_tz":0,"elapsed":917,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"b0660497-7a1a-4d85-d05a-deefd4711f75"},"source":["import nltk\r\n","import re\r\n","from nltk.tokenize import sent_tokenize, word_tokenize\r\n","nltk.download('stopwords')\r\n","nltk.download('punkt')\r\n","nltk.download('averaged_perceptron_tagger')\r\n","from tqdm import tqdm\r\n","from nltk.corpus import stopwords\r\n","stopwordEn = stopwords.words('english')\r\n","from nltk.corpus import wordnet\r\n","nltk.download('wordnet')\r\n","from nltk.stem.snowball import SnowballStemmer\r\n","stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\r\n","\r\n","#define Lemmatisation\r\n","\r\n","def stemWord(word):\r\n","    stem = stemmer.stem(word)\r\n","    if stem is not None:\r\n","        return stem\r\n","    else:\r\n","        return word\r\n","\r\n","def processText(text,lemma=False, gram=1, rmStop=True): # default remove stop words\r\n","    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE) #delete URL, #hashtag# , and @xxx\r\n","    tokens = word_tokenize(text)\r\n","    whitelist = [\"n't\", \"not\", \"no\"]\r\n","    new_tokens = []\r\n","    stoplist = stopwordEn if rmStop else []\r\n","    for i in tokens:\r\n","      i = i.lower()\r\n","      if i.isalpha() and (i not in stoplist or i in whitelist):  #i not in ['.',',',';']  and (...)\r\n","        if lemma: i = lemmaWord(i)\r\n","        new_tokens.append(i)\r\n","    del tokens\r\n","    # tokens = [lemmaWord(i.lower()) if lemma else i.lower() for i in tokens if (i.lower() not in stoplist or i.lower() in whitelist) and i.isalpha()]\r\n","    if gram<=1:\r\n","        return new_tokens\r\n","    else:\r\n","        return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]\r\n","\r\n","def getTags(text):\r\n","  token = word_tokenize(text)\r\n","  token = [l.lower() for l in token]\r\n","  train_tags = nltk.pos_tag(token)\r\n","  return [i[1] for i in train_tags]\r\n","\r\n","# define your own NLP techniques here\r\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YKFoZaWSVrnq"},"source":["\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8mwYOcFcS02","executionInfo":{"status":"ok","timestamp":1615032049633,"user_tz":0,"elapsed":402,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"807dd69a-44eb-4a29-db14-5ef7a30a8e0f"},"source":["print(processText(twenty_train.data[0]))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["['michael', 'collier', 'subject', 'converting', 'images', 'hp', 'laserjet', 'iii', 'hampton', 'organization', 'city', 'university', 'lines', 'anyone', 'know', 'good', 'way', 'standard', 'pc', 'utility', 'convert', 'files', 'laserjet', 'iii', 'format', 'would', 'also', 'like', 'converting', 'hpgl', 'hp', 'plotter', 'files', 'please', 'email', 'response', 'correct', 'group', 'thanks', 'advance', 'michael', 'michael', 'collier', 'programmer', 'computer', 'unit', 'email', 'city', 'university', 'tel', 'london', 'fax']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZRqySa8cbr1","executionInfo":{"status":"ok","timestamp":1615032046676,"user_tz":0,"elapsed":588,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"5ac36e9d-41a1-4d43-b838-9b893b22bb29"},"source":["print(getTags(twenty_train.data[0]))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["['IN', ':', 'NN', 'NN', 'NN', '(', 'FW', 'NN', ')', 'NN', ':', 'NN', 'NNS', 'TO', 'VB', 'NN', 'NN', '.', 'NN', ':', 'NN', 'NN', ':', 'DT', 'NN', 'NN', 'NNS', ':', 'CD', 'VBZ', 'NN', 'VB', 'IN', 'DT', 'JJ', 'NN', '(', 'JJ', 'NN', 'JJ', 'NN', ')', 'TO', 'VB', 'JJ', 'NNS', 'IN', 'NN', 'JJ', 'NN', '.', 'PRP', 'MD', 'RB', 'VB', 'TO', 'VB', 'DT', 'JJ', ',', 'VBG', 'TO', 'VB', '(', 'JJ', 'NN', ')', 'NNS', '.', 'VB', 'JJ', 'DT', 'NN', '.', 'VBZ', 'DT', 'DT', 'JJ', 'NN', '.', 'NNS', 'IN', 'NN', '.', 'NN', '.', ':', 'JJ', 'NN', '(', 'NN', ')', 'DT', 'NN', 'NN', ',', 'NN', ':', 'NN', 'VBZ', 'IN', 'DT', 'NN', 'NN', ',', 'NN', ':', 'CD', 'CD', 'JJ', 'NN', ',', 'NN', ':', 'CD', 'JJ', 'NN', 'CD', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"44xTvpLa_UC9"},"source":["# Step 2: Build a Pipeline"]},{"cell_type":"code","metadata":{"id":"vNm3axlhdzlF"},"source":["from sklearn.pipeline import Pipeline\r\n","from sklearn.feature_extraction.text import CountVectorizer\r\n","from sklearn.feature_extraction.text import TfidfTransformer\r\n","from sklearn.linear_model import SGDClassifier, LogisticRegression"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PO21oPjEeTsP"},"source":["## Examples of CountVectorizer, TfidfTransformer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"7YwO7dNKd0o8","executionInfo":{"status":"error","timestamp":1615032061347,"user_tz":0,"elapsed":394,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"dff4e00d-0fb9-41f1-8151-99d07d9c7bea"},"source":["count_vect = CountVectorizer(analyzer='word', ngram_range=(1, 1))\r\n","example_count_vect = count_vect.fit_transform(twenty_train.data[:10])\r\n","example_count_vect.toarray()[0]"],"execution_count":11,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-ca06f4f651d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexample_count_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexample_count_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ymEYTp-ifLfp","executionInfo":{"status":"ok","timestamp":1614949447298,"user_tz":0,"elapsed":461,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"c9ec3644-13de-4e93-9f35-ad9d9d43a44a"},"source":["pd.Series(example_count_vect.toarray()[0]).value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    977\n","1     57\n","2     11\n","4      2\n","3      2\n","5      1\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":118}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfmOlRH1igE1","executionInfo":{"status":"ok","timestamp":1614950324899,"user_tz":0,"elapsed":714,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"2c2328aa-cd7d-4115-9dc5-42779278876a"},"source":["tfidf_transformer = TfidfTransformer()\r\n","example_tfidf = tfidf_transformer.fit_transform(example_count_vect)\r\n","example_tfidf.toarray()[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.        , 0.        , 0.18288121, ..., 0.        , 0.        ,\n","       0.        ])"]},"metadata":{"tags":[]},"execution_count":131}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sT4t5d8MfAjX","executionInfo":{"status":"ok","timestamp":1614950327246,"user_tz":0,"elapsed":396,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"69dc061b-84a8-4701-dc9e-360bc0d0d6f6"},"source":["pd.Series(example_tfidf.toarray()[0]).value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.000000    977\n","0.091441     29\n","0.033807      7\n","0.182881      7\n","0.068007      7\n","0.077733      6\n","0.060463      4\n","0.054299      3\n","0.155466      3\n","0.274322      2\n","0.120926      1\n","0.148119      1\n","0.169037      1\n","0.037030      1\n","0.365762      1\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"tLqTENPlf8de","executionInfo":{"status":"ok","timestamp":1614949722323,"user_tz":0,"elapsed":793,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"13f3cc42-cc9c-4fc7-e6d4-994d809da488"},"source":["pd.DataFrame({'word':list(count_vect.vocabulary_.keys())\r\n","              ,'CountVector':count_vect.vocabulary_.values()\r\n","              ,'tf_idf_value':tfidf_transformer.idf_})"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>CountVector</th>\n","      <th>tf_idf_value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>from</td>\n","      <td>381</td>\n","      <td>2.299283</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sd345</td>\n","      <td>805</td>\n","      <td>2.299283</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>city</td>\n","      <td>199</td>\n","      <td>2.704748</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ac</td>\n","      <td>44</td>\n","      <td>2.704748</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>uk</td>\n","      <td>954</td>\n","      <td>2.299283</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1045</th>\n","      <td>colleague</td>\n","      <td>207</td>\n","      <td>1.788457</td>\n","    </tr>\n","    <tr>\n","      <th>1046</th>\n","      <td>knows</td>\n","      <td>534</td>\n","      <td>2.299283</td>\n","    </tr>\n","    <tr>\n","      <th>1047</th>\n","      <td>type</td>\n","      <td>950</td>\n","      <td>2.704748</td>\n","    </tr>\n","    <tr>\n","      <th>1048</th>\n","      <td>wants</td>\n","      <td>987</td>\n","      <td>2.704748</td>\n","    </tr>\n","    <tr>\n","      <th>1049</th>\n","      <td>afraid</td>\n","      <td>57</td>\n","      <td>2.704748</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1050 rows × 3 columns</p>\n","</div>"],"text/plain":["           word  CountVector  tf_idf_value\n","0          from          381      2.299283\n","1         sd345          805      2.299283\n","2          city          199      2.704748\n","3            ac           44      2.704748\n","4            uk          954      2.299283\n","...         ...          ...           ...\n","1045  colleague          207      1.788457\n","1046      knows          534      2.299283\n","1047       type          950      2.704748\n","1048      wants          987      2.704748\n","1049     afraid           57      2.704748\n","\n","[1050 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":129}]},{"cell_type":"markdown","metadata":{"id":"ICTNFePreVgH"},"source":["## Build Pipeline"]},{"cell_type":"code","metadata":{"id":"HQzwithdCl9e","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"error","timestamp":1615032105083,"user_tz":0,"elapsed":434,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"106c81a7-d44c-4ffd-89f6-9fd1ba70af95"},"source":["# Level: lexicom, model: tf-idf\n","text_clf = Pipeline([\n","    # add your code about text processing           \n","    ('vect', CountVectorizer(analyzer=processText)), \n","    ('tfidf', TfidfTransformer(use_idf=True)),\n","\n","    # change your classifier here, for example SGDClassifier\n","    ('clf', SGDClassifier())\n","\n","])\n"],"execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-796f21f4ea95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Level: lexicom, model: tf-idf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m text_clf = Pipeline([\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# add your code about text processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'vect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":181},"id":"Vuq37Bf3Qjpn","executionInfo":{"status":"error","timestamp":1615032116870,"user_tz":0,"elapsed":403,"user":{"displayName":"Huizhi Liang","photoUrl":"","userId":"06861689905349463303"}},"outputId":"c9eb4896-6e22-401d-8dc6-f425e4e1156b"},"source":["# To train the model \r\n","text_clf.fit(twenty_train.data, twenty_train.target)"],"execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-e0a1a411b7fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# To train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwenty_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'text_clf' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"0uxJCHfCYwQh"},"source":["# Other Models"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RDjZkkvfXKzq","executionInfo":{"status":"ok","timestamp":1614947664238,"user_tz":0,"elapsed":39660,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"42616991-1352-4b67-ef24-13c4cd93b119"},"source":["# Level: syntax, pos tags, model: tf-idf\r\n","text_clf_postag = Pipeline([\r\n","    # add your code about text processing           \r\n","    ('vect', CountVectorizer(analyzer=getTags)), \r\n","    ('tfidf', TfidfTransformer(use_idf=True)),\r\n","    ('clf', LogisticRegression())\r\n","\r\n","])\r\n","# To train the model \r\n","text_clf_postag.fit(twenty_train.data, twenty_train.target)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 CountVectorizer(analyzer=<function getTags at 0x7f6434747b90>,\n","                                 binary=False, decode_error='strict',\n","                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n","                                 input='content', lowercase=True, max_df=1.0,\n","                                 max_features=None, min_df=1,\n","                                 ngram_range=(1, 1), preprocessor=None,\n","                                 stop_words=None, strip_accents=None,\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 to...ne)),\n","                ('tfidf',\n","                 TfidfTransformer(norm='l2', smooth_idf=True,\n","                                  sublinear_tf=False, use_idf=True)),\n","                ('clf',\n","                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n","                                    fit_intercept=True, intercept_scaling=1,\n","                                    l1_ratio=None, max_iter=100,\n","                                    multi_class='auto', n_jobs=None,\n","                                    penalty='l2', random_state=None,\n","                                    solver='lbfgs', tol=0.0001, verbose=0,\n","                                    warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ne5gP00X0Ss","executionInfo":{"status":"ok","timestamp":1614947692839,"user_tz":0,"elapsed":61850,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"1c246453-83ff-408d-9d9b-219da8381507"},"source":["predicted = text_clf_postag.predict(twenty_test.data)\r\n","print(\"Accuracy:\", metrics.accuracy_score(twenty_test.target, predicted))\r\n","print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.5346205059920106\n","                        precision    recall  f1-score   support\n","\n","           alt.atheism       0.55      0.39      0.45       319\n","         comp.graphics       0.57      0.67      0.62       389\n","               sci.med       0.48      0.36      0.41       396\n","soc.religion.christian       0.52      0.70      0.60       398\n","\n","              accuracy                           0.53      1502\n","             macro avg       0.53      0.53      0.52      1502\n","          weighted avg       0.53      0.53      0.52      1502\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gjQ8DmPNRUuJ"},"source":["# Make Prediction"]},{"cell_type":"code","metadata":{"id":"OMdoIHjMRWce"},"source":["# To make prediction with dev/test set\r\n","predicted = text_clf.predict(twenty_test.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fuzn1IcAU2-B"},"source":["# save your predictin to file\r\n","pd.Series(predicted).to_csv('res.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GXHJHqoBmyJ"},"source":["# Step 3: Evaluation"]},{"cell_type":"code","metadata":{"id":"LdB9js0QDErf","colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"status":"ok","timestamp":1614946280237,"user_tz":0,"elapsed":9414,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"4adb7b8d-5024-4947-ce90-fbc59dacd313"},"source":["# To evaluate your prediction on dev set\n","from sklearn import metrics\n","print(\"Accuracy:\", metrics.accuracy_score(twenty_test.target, predicted))\n","\n","print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n","\n","# confusion class\n","pd.DataFrame(metrics.confusion_matrix(twenty_test.target, predicted),\n","             columns=twenty_test.target_names,index=twenty_test.target_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.9181091877496671\n","                        precision    recall  f1-score   support\n","\n","           alt.atheism       0.91      0.81      0.86       319\n","         comp.graphics       0.94      0.97      0.95       389\n","               sci.med       0.95      0.94      0.94       396\n","soc.religion.christian       0.88      0.94      0.91       398\n","\n","              accuracy                           0.92      1502\n","             macro avg       0.92      0.91      0.91      1502\n","          weighted avg       0.92      0.92      0.92      1502\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>alt.atheism</th>\n","      <th>comp.graphics</th>\n","      <th>sci.med</th>\n","      <th>soc.religion.christian</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>alt.atheism</th>\n","      <td>258</td>\n","      <td>5</td>\n","      <td>10</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>comp.graphics</th>\n","      <td>6</td>\n","      <td>376</td>\n","      <td>6</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>sci.med</th>\n","      <td>8</td>\n","      <td>13</td>\n","      <td>371</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>soc.religion.christian</th>\n","      <td>12</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>374</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                        alt.atheism  ...  soc.religion.christian\n","alt.atheism                     258  ...                      46\n","comp.graphics                     6  ...                       1\n","sci.med                           8  ...                       4\n","soc.religion.christian           12  ...                     374\n","\n","[4 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"qCLCqFXPQsRq"},"source":["# Step 4: Error Analysis"]},{"cell_type":"code","metadata":{"id":"kvBw9qkKDS-m","colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"status":"ok","timestamp":1614946474395,"user_tz":0,"elapsed":780,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"656bca0d-5937-4025-ae8b-d4fb52d4bf9a"},"source":["df_pred = pd.DataFrame({'news':twenty_test.data,'prediction':predicted, 'true':twenty_test.target})\n","df_pred[df_pred['true'] != df_pred['prediction']]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>news</th>\n","      <th>prediction</th>\n","      <th>true</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8</th>\n","      <td>From: GWGREG01@ukcc.uky.edu\\nSubject: Re: Preg...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>From: \"Gabriel D. Underwood\" &lt;gabe+@CMU.EDU&gt;\\n...</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>From: swf@elsegundoca.ncr.com (Stan Friesen)\\n...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>From: mathew &lt;mathew@mantis.co.uk&gt;\\nSubject: D...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>From: UC512052@mizzou1.missouri.edu (David K. ...</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1438</th>\n","      <td>From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1450</th>\n","      <td>From: alan.barclay@almac.co.uk (Alan Barclay)\\...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1455</th>\n","      <td>From: pww@spacsun.rice.edu (Peter Walker)\\nSub...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1480</th>\n","      <td>From: wilsonr@logica.co.uk\\nSubject: Re: What ...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1492</th>\n","      <td>From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>123 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                   news  prediction  true\n","8     From: GWGREG01@ukcc.uky.edu\\nSubject: Re: Preg...           0     2\n","12    From: \"Gabriel D. Underwood\" <gabe+@CMU.EDU>\\n...           3     2\n","15    From: swf@elsegundoca.ncr.com (Stan Friesen)\\n...           2     0\n","19    From: mathew <mathew@mantis.co.uk>\\nSubject: D...           3     0\n","26    From: UC512052@mizzou1.missouri.edu (David K. ...           2     1\n","...                                                 ...         ...   ...\n","1438  From: mangoe@cs.umd.edu (Charley Wingate)\\nSub...           0     3\n","1450  From: alan.barclay@almac.co.uk (Alan Barclay)\\...           1     2\n","1455  From: pww@spacsun.rice.edu (Peter Walker)\\nSub...           3     0\n","1480  From: wilsonr@logica.co.uk\\nSubject: Re: What ...           3     0\n","1492  From: kmr4@po.CWRU.edu (Keith M. Ryan)\\nSubjec...           3     0\n","\n","[123 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"UMtYeEczdkT2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QI-KUNM9dkQP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFesFLgBdkMe"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4v4a-vZsk1wL"},"source":["# Word2Vec Classification\r\n","use 20newsgroups dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1YkrWzijlv45","executionInfo":{"status":"ok","timestamp":1614867320258,"user_tz":0,"elapsed":12204,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"77d82b95-90c5-417f-fc2a-acef15acb55f"},"source":["from sklearn.datasets import fetch_20newsgroups\r\n","\r\n","categories = ['soc.religion.christian','comp.graphics', 'sci.med']\r\n","twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading 20news dataset. This may take a few minutes.\n","Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"_OVBJ7s0l8qe","executionInfo":{"status":"ok","timestamp":1614874640100,"user_tz":0,"elapsed":5664,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"ff681bd5-ae0f-4748-dffe-7a63cd83e3e2"},"source":["import pandas as pd\r\n","\r\n","df = pd.DataFrame({'text':twenty_train.data,'class':twenty_train.target})\r\n","df.loc[:,'tokens'] = [processText(i) for i in tqdm(df['text'])]\r\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1777/1777 [00:05<00:00, 340.45it/s]\n","/usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/missing.py:495: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  val = np.array(val, copy=False)\n","/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  return array(a, dtype, copy=False, order=order)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>class</th>\n","      <th>tokens</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>From: tbrent@ecn.purdue.edu (Timothy J Brent)\\...</td>\n","      <td>2</td>\n","      <td>[from, timothy, j, brent, subject, am, i, goin...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>From: CBW790S@vma.smsu.edu.Ext (Corey Webb)\\nS...</td>\n","      <td>0</td>\n","      <td>[from, corey, webb, subject, re, help, grasp, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>From: ds@aris.nswc.navy.mil (Demetrios Sapouna...</td>\n","      <td>0</td>\n","      <td>[from, demetrios, sapounas, subject, display, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Nntp-Posting-Host: bones.et.byu.edu\\nLines: 6\\...</td>\n","      <td>0</td>\n","      <td>[lines, subject, pd, viewer, wanted, summary, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Organization: Penn State University\\nFrom: &lt;JE...</td>\n","      <td>0</td>\n","      <td>[organization, penn, state, university, from, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  ...                                             tokens\n","0  From: tbrent@ecn.purdue.edu (Timothy J Brent)\\...  ...  [from, timothy, j, brent, subject, am, i, goin...\n","1  From: CBW790S@vma.smsu.edu.Ext (Corey Webb)\\nS...  ...  [from, corey, webb, subject, re, help, grasp, ...\n","2  From: ds@aris.nswc.navy.mil (Demetrios Sapouna...  ...  [from, demetrios, sapounas, subject, display, ...\n","3  Nntp-Posting-Host: bones.et.byu.edu\\nLines: 6\\...  ...  [lines, subject, pd, viewer, wanted, summary, ...\n","4  Organization: Penn State University\\nFrom: <JE...  ...  [organization, penn, state, university, from, ...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":234}]},{"cell_type":"markdown","metadata":{"id":"HgD4MYBanxe7"},"source":["## GLOVE\r\n","example of loading pre-trained embedding "]},{"cell_type":"code","metadata":{"id":"czKDpnMIoJYx"},"source":["# \r\n","EMB_DIM = 50\r\n","glove = {}\r\n","with open(PATH+F\"./GLOVE/glove.twitter.27B.{EMB_DIM}d.txt\", 'rb') as f:\r\n","    for line in f:\r\n","        values = line.split()\r\n","        word = values[0]\r\n","        vector = np.asarray(values[1:], \"float32\")\r\n","        if len(vector)==EMB_DIM:\r\n","            glove[word] = vector\r\n","print('Loaded %s word vectors.' % len(glove))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SPFod_6tnzgH"},"source":["## W2V\r\n","or train a Word2Vec model from scratch "]},{"cell_type":"code","metadata":{"id":"FuXCu0VbnWcy"},"source":["import gensim\r\n","model = gensim.models.Word2Vec(df['tokens'], size=100)\r\n","w2v = dict(zip(model.wv.index2word, model.wv.vectors))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PM4P9PzptTw","executionInfo":{"status":"ok","timestamp":1614874667563,"user_tz":0,"elapsed":4191,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"cf1253f7-76f1-49f8-ae1b-19bfe4563d86"},"source":["len(w2v) # w2f shape: 7414 * 100"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7544"]},"metadata":{"tags":[]},"execution_count":236}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAoKLNSmsnc0","executionInfo":{"status":"ok","timestamp":1614870820958,"user_tz":0,"elapsed":913,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"16f0d96d-acff-4e2d-f882-f560ff0282f0"},"source":["# count vec + raw text\r\n","count_vect = CountVectorizer(analyzer='word', ngram_range=(1, 1))\r\n","X_train_counts = count_vect.fit_transform(twenty_train.data)\r\n","X_train_counts.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1777, 31638)"]},"metadata":{"tags":[]},"execution_count":146}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4vLcmzDWvwsH","executionInfo":{"status":"ok","timestamp":1614870608478,"user_tz":0,"elapsed":642,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"927cafed-9bcd-41f2-e1a1-b753acfcd2c0"},"source":["# count vec + prepared tokens\r\n","count_vect = CountVectorizer(analyzer=lambda x: x, ngram_range=(1, 1))\r\n","X_train_counts = count_vect.fit_transform(df['tokens'])\r\n","X_train_counts.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1777, 23972)"]},"metadata":{"tags":[]},"execution_count":143}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a36Kb8Uuy9ui","executionInfo":{"status":"ok","timestamp":1614870852340,"user_tz":0,"elapsed":565,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"9a9a3892-8b22-4307-f204-485d43f47689"},"source":["# tfidf vec + prepared tokens\r\n","count_vect = TfidfVectorizer(analyzer=lambda x: x)\r\n","X_train_counts = count_vect.fit_transform(df['tokens'])\r\n","X_train_counts.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1777, 23972)"]},"metadata":{"tags":[]},"execution_count":148}]},{"cell_type":"code","metadata":{"id":"xofIMPJOn-q4"},"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n","from collections import defaultdict\r\n","class MeanEmbeddingVectorizer(object): # to replace the basic CountVectorizer\r\n","    def __init__(self, word2vec):\r\n","        self.word2vec = word2vec\r\n","        # if a text is empty we should return a vector of zeros\r\n","        # with the same dimensionality as all the other vectors\r\n","        self.dim = len(word2vec)\r\n","\r\n","    def fit(self, X, y):\r\n","        return self\r\n","\r\n","    def transform(self, X):\r\n","        return np.array([\r\n","            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\r\n","                    or [np.zeros(self.dim)], axis=0)\r\n","            for words in X\r\n","        ])\r\n","\r\n","class TfidfEmbeddingVectorizer(object): # tf-idf weights * mean(word2vec)\r\n","    def __init__(self, word2vec):\r\n","        self.word2vec = word2vec\r\n","        self.word2weight = None\r\n","        self.dim = len(word2vec)\r\n","\r\n","    def fit(self, X, y):\r\n","        tfidf = TfidfVectorizer(analyzer=lambda x: x)\r\n","        tfidf.fit(X)\r\n","        # if a word was never seen - it must be at least as infrequent\r\n","        # as any of the known words - so the default idf is the max of \r\n","        # known idf's\r\n","        max_idf = max(tfidf.idf_)\r\n","        self.word2weight = defaultdict(\r\n","            lambda: max_idf,\r\n","            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\r\n","\r\n","        return self\r\n","\r\n","    def transform(self, X):\r\n","        return np.array([\r\n","                np.mean([self.word2vec[w] * self.word2weight[w]\r\n","                         for w in words if w in self.word2vec] or\r\n","                        [np.zeros(self.dim)], axis=0)\r\n","                for words in X\r\n","            ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6jG53-QqTCn"},"source":["from sklearn.pipeline import Pipeline\r\n","from sklearn.ensemble import ExtraTreesClassifier\r\n","\r\n","etree_w2v = Pipeline([\r\n","    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\r\n","    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\r\n","etree_w2v_tfidf = Pipeline([\r\n","    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\r\n","    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0IpaJxYirAsz","executionInfo":{"status":"ok","timestamp":1614874676547,"user_tz":0,"elapsed":1614,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"9b115701-9093-46db-dc7a-3589962f0c78"},"source":["etree_w2v.fit(df['tokens'], df['class'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('word2vec vectorizer',\n","                 <__main__.MeanEmbeddingVectorizer object at 0x7f663ae2cad0>),\n","                ('extra trees',\n","                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n","                                      class_weight=None, criterion='gini',\n","                                      max_depth=None, max_features='auto',\n","                                      max_leaf_nodes=None, max_samples=None,\n","                                      min_impurity_decrease=0.0,\n","                                      min_impurity_split=None,\n","                                      min_samples_leaf=1, min_samples_split=2,\n","                                      min_weight_fraction_leaf=0.0,\n","                                      n_estimators=200, n_jobs=None,\n","                                      oob_score=False, random_state=None,\n","                                      verbose=0, warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":239}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ERlZJ2zsPFq","executionInfo":{"status":"ok","timestamp":1614874685483,"user_tz":0,"elapsed":9466,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"367a2d4a-a758-4ef7-cb96-6d5d936febd2"},"source":["etree_w2v_tfidf.fit(df['tokens'], df['class'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('word2vec vectorizer',\n","                 <__main__.TfidfEmbeddingVectorizer object at 0x7f663ae2cd90>),\n","                ('extra trees',\n","                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n","                                      class_weight=None, criterion='gini',\n","                                      max_depth=None, max_features='auto',\n","                                      max_leaf_nodes=None, max_samples=None,\n","                                      min_impurity_decrease=0.0,\n","                                      min_impurity_split=None,\n","                                      min_samples_leaf=1, min_samples_split=2,\n","                                      min_weight_fraction_leaf=0.0,\n","                                      n_estimators=200, n_jobs=None,\n","                                      oob_score=False, random_state=None,\n","                                      verbose=0, warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":240}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeynA5ghtWa_","executionInfo":{"status":"ok","timestamp":1614874689633,"user_tz":0,"elapsed":11699,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"ba8225e6-762a-42b3-d5ef-47a8d391d2fc"},"source":["# import test set\r\n","twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\r\n","X_test = [processText(i) for i in tqdm(twenty_test.data)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 1183/1183 [00:03<00:00, 304.64it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHE8Dqgq0SIz","executionInfo":{"status":"ok","timestamp":1614871369435,"user_tz":0,"elapsed":756,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"8df8307b-972b-4641-eed8-017a337d01a9"},"source":["from sklearn import metrics\r\n","predicted = etree_w2v.predict(X_test) # make prediction\r\n","print(\"Accuracy:\",np.mean(predicted == twenty_test.target)) \r\n","print(\"F1:\",metrics.classification_report(twenty_test.target, predicted)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.7734573119188504\n","F1:               precision    recall  f1-score   support\n","\n","           0       0.76      0.87      0.81       389\n","           1       0.72      0.63      0.67       396\n","           2       0.84      0.82      0.83       398\n","\n","    accuracy                           0.77      1183\n","   macro avg       0.77      0.77      0.77      1183\n","weighted avg       0.77      0.77      0.77      1183\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pir5G-Sa1LjF","executionInfo":{"status":"ok","timestamp":1614871393856,"user_tz":0,"elapsed":1035,"user":{"displayName":"MarshallCN","photoUrl":"","userId":"04885756257830296956"}},"outputId":"49466e58-bda3-4335-9729-7b49c06f29a4"},"source":["predicted = etree_w2v_tfidf.predict(X_test) # make prediction\r\n","print(\"Accuracy:\",np.mean(predicted == twenty_test.target)) \r\n","print(\"F1:\",metrics.classification_report(twenty_test.target, predicted)) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.8064243448858833\n","F1:               precision    recall  f1-score   support\n","\n","           0       0.79      0.87      0.83       389\n","           1       0.74      0.71      0.73       396\n","           2       0.89      0.83      0.86       398\n","\n","    accuracy                           0.81      1183\n","   macro avg       0.81      0.81      0.81      1183\n","weighted avg       0.81      0.81      0.81      1183\n","\n"],"name":"stdout"}]}]}